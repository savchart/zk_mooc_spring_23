Letâ€™s build an efficient SNARKAs a reminder most snarks are designed by combining two componentsa cryptographic protocol called a polynomial commitment scheme and ainteractive protocol called a polynomial interactive Oracle proof or a polynomialIOP and you can kind of take any polynomial IOP and any polynomial commitment scheme with some minor caveats and putthem together to get a interactive argument that is succinctand then apply the Fiat Shamir transformation to render the succinctinteractive argument non-interactive and thereby obtain a snark so at this pointin the course we have seen a variety of differentpolynomial iops and a variety of different polynomial commitment schemesand each individually has sort of trade-offs you know a unique performanceprofile and pros and cons and the combinations of polynomial iops and polynomial commitments also inherit thecorresponding pros and cons so I thought I would give a very brief summaryof the various performance profiles obtainable from the protocolsthat the course has already covered and the protocols that we have yet to coverRecall: What is a Polynomial-IOP?as a brief reminder what is a polynomial IOP and what is a polynomial equipment schemeso I will tell you actually a special case of a polynomial IOP that suffices for most but not all snarks so in thisspecial case of a polynomial IOP the prover's first message to the verifierspecifies a polynomial H the polynomial H you should think of as very large sofor example if it is a univariate polynomial its degree might be as big asthe circuit that the prover is claiming to know a satisfying assignment for soas a result of the polynomial being so large we don't want the verifier to haveto read a complete description of that polynomial for example we don't want the verifier to have to read everycoefficient of that polynomial. theverifier would simply be too slow so we will in the polynomial IOP not allow theverifier to learn H in full rather the verifier will only bepermitted to evaluate H at say a single point of the verifier's choosingand after the verifier evaluates H at that point the prover and verifierthen execute a standard interactive proof meaning other than that very first specialmessage that specified the polynomial H every other message from the prover isshort and read by the verifier in full right and that then at the end of the polynomial IOP on the verifier decideswhether to accept or reject the prover's claim as validRecall: What is a Polynomial Commitment Scheme?a polynomial commitment scheme is a cryptographic protocol that is essentially exactly what we would needto turn a polynomial IOP into a succinct interactive argument so specifically apolynomial commitment scheme allows the prover to more or less simulate thepolynomial IOP without explicitly sending a description of this large polynomial H to theverifier so instead the polynomial commitment scheme will let the prover send a succinct cryptographic commitment to Hto the verifier typically this will just be a single cryptographic hash value ora single element of a cryptographic group and the polynomial commitmentscheme has to support the functionality of allowing the verifier to later choosean input x to the polynomial H and demand that the prover evaluate thecommitted polynomial H at X and then convince the verifier that indeed theevaluation that the prover returns to the verifier is equal to the committedpolynomial evaluated at X so we call that an evaluation proof for thepolynomial commitment scheme okay and the idea is that this just allows one touse a polynomial IOP without requiring the verifier to explicitly receive acomplete description of the polynomial rather the verifier will receive justessentially a single hash value or cryptographic group element that commits to the polynomial and later it willreceive the requested evaluation of the polynomial and a proof that the requested evaluation is correct so iteffectively allows one to run a polynomial IOP succinctly meaning without the prover ever sending this onegiant message specifying the polynomial H explicitly to the verifierA Zoo of SNARKsokay so as I've already reviewed there are several different polynomial iops out there and several differentfrom polynomial commitments and you can kind of take any polynomial IOP inany polynomial commitment and put them together up to some minor caveats and it's always worth reminding you thatthe resulting snark inherits the property of being transparent or notor of being plausibly post Quantum secure or not entirely from the polynomial commitment scheme used so ifI tell you what polynomial commitment scheme the snark is using you will immediately know is the snark transparentversus does it require atrusted setup is it possibly post Quantum secure or is it obviously not post Quantum secure okayPolynomial IOPs: Three classesnow a very brief overview of known polynomial iops so I I categorize them into into three different categories one isbased on what are called interactive proofs the others based on what are called multi-prover interactive proofsessentially both of these categories were covered in lecture four the first lecture that I gave in this moocand then the third category according to my taxonomy is what I call constant round polynomial iopsand the example the the most prominent examples here are Marlin and Plonk and and so Dan covered the Plonkpolynomial IOP I think in lecture five and and very roughly speaking theabove snarks are listed in increasing order of a prover costs butdecreasing order of verification costs okay so snarks based on interactive person multi-interactive proofs broadlyspeaking have the fastest prover but higher verification costs than snarks like Marlin and Plonk everything ofcourse does ultimately depend as well on what polynomial commitment scheme you combine the polynomial IOP with Marlinand Plonk both typically use kzg polynomial commitments which keeps the verification costs really low becausekzg polynomial commitments are is is the polynomial commitment scheme with the best known verification costsokay in terms of polynomial commitment schemes that people use in practice today I categorize them into threeclasses I think this is a pretty common way of classifying known polynomial commitmentsthe first category is based on pairing friendly groups and trusted setup meaning that such polynomialcommitments are neither transparent not post Quantum secure because anythingthat uses a cryptographic group is not post Quantum and anything that uses a trusted setup is by definition nottransparent the most important example in this category is kzg commitments which werecovered in both lectures five and six and the the unique property that kzgcommitments have is is their their evaluation proofs and the commitment itself is just a constant number ofelements of a pairing friendly cryptographic group so we call that constant size proofs and they are thesecommitments kzg commitments are unique in achieving thatthe second category of polynomial commitments is based on the discrete logarithm problem and do not require a trustedsetup so these will be transparent but they'renot post Quantum secure again because the discrete logarithm problem can be solved in polynomial time if we had a alarge enough quantum computer examples in this category include IPAalso known as bulletproofs which upen covered in lecture six hyrax and DoryDory does use pairing friendly groups but it is transparent unlike kzg polynomial commitments you could thinkof Dory as roughly taking bulletproofs which has logarithmic size proofs butlinear verification time and addressing the linear verification time reducing itto logarithmic verification time at the cost of needing to use pairing friendlygroups whereas bulletproof does not and also at a concrete cost even though theproof length is logarithmic the Dory evaluation proofs are concretely substantially bigger than thebulletproof's evaluation proofs even though asymptotically they're the sameand then the third category ofpolynomial commitments that people use today is based on hashing so the only cryptography they use is Merkelhashing and the Fiat Shamir transformation both of which only use cryptographic hash functionsso this category is both transparent and plausibly post Quantumsecure The prominent examples in this category the the most prominent one is FRI that's because it is the example inthis category with the shortest our evaluation proofs that will be the subject of today's lecture and we sawother examples in this category last lecture lecture seven these include Ligero breakdown and Orionso broadly speaking here the the three categories I havelisted in increasing order of verification costs so kzg commitmentshave constant verification costs things like bulletproof and Dory have logarithmic verification costs andprotocols like FRI and Orion have poly logarithmic verification costs so thatis at least a log squared size proofs with a significant dependence on thesecurity parameter as well so it's like security parameter Times log squared ofthe size of the polynomial being committed in this third category for FRI okay so another thing worth mentioningis that commitment schemes from the first two categories that is using cryptographic groups are homomorphic sothis means you can take a commitment to polynomial one and a commitment to polynomial 2 and without knowing whatpolynomial one is or what polynomial 2 is you can kind of add the commitments together to get a commitment topolynomial 1 plus polynomial two so you can kind of do addition underneath thecommitments right again you can take two commitments and derive a commitment to the sum of the two committed objectswithout actually knowing either of the committed objects so these homomorphism properties of discrete logarithm andpairing based polynomial commitments are really important for fishing batching and amortization properties with snarksso this means on things like you know a prover is proving to know manydifferent Witnesses and doesn't want to have to sort of pay a cost that growslinearly with the number of witnesses it's proving that it knows and these homomorphism properties without goinginto great detail right now allow the prover to kind of prove that it knows 10 different Witnesses with costs that arenot necessarily 10 times greater than proving that it knows a single witness and these sorts of homomorphismproperties are likely to come up two lectures from now when Dan talks about exactly these topics so snarkcomposition batching and amortization okay so the the polynomial commitmentsbased on cryptographic hashing alone are not homomorphic so they are not quite asnice if you're working in a setting where there's opportunities for batching and amortizationSome specimens from the zoookay so again you can take any polynomial IOP in any polynomial commitment scheme and with some modestcaveats put them together to get a snark or at least a succinct interactive argument that you can make into a snarkwith Fiat Shamir so let me just Briefly summarize some particularsnarks that are especially popular right now and explain what their pros and cons are just to try to make thisbig zoo of snarks that you can get a little more concreteHighlights of SNARK Taxonomy: Transparent SNARKsHalo2okay so I'll start with transparentsnarks so one very popular implementation of transmit parent snarks today is Halo 2I've written here Halo 2 hyphen zcash basically to indicate the Halo 2 codebase with bulletproof slash IPA as the polynomial commitment scheme becausethere's a different branch of the Halo 2 code base that swaps out IPA slash bulletproofs for kzg in order to takeadvantage of you know smaller proofs and things like that at the cost of a trusted setup and by the way this isessentially the Plonk polynomial IOP with the bulletproof polynomial commitment schemethe benefit especially of using bulletproofs as the polynomial commitment is that this obtains theshortest proofs among transparent snarks out there todaythe downside is especially in a non-amortized setting the verifier is very slow linear time in the size ofthe quote circuit that the prover is claiming to know a satisfying assignment for so this is not a work saving verifierbut it is a succinct proof this is one reason I like to separate the notion of short proofs and fastverifiers right because people do call Halo 2 a snark in fact in anon-amortized setting it is not work saving for the verifier so apparently inthis context the words succinct is referring only to proof length and notto verification timeHighlights of SNARK Taxonomy: Transparent SNARKsSTARKs, Fractal, Aurora, Virgo, Ligero+okay another example is you canactually take any polynomial IOP and combine it with fry based polynomial commitments and examples here include whatpeople call Starks fractal Aurora Virgo Ligero plus plus and Ligero itself is a is a a polynomial commitment schemein the same category as forever with bigger proofs the hero plus plus kind of combines the little hero polynomialcoming in this game with the FRI polynomial commitment scheme to kind of get the nice prover cost of Ligero but theshorter proofs of FRI so the pros of snarks from this category is because they use FRI as thepolynomial commitment they obtain the shortest proofs amongst plausibly post Quantum snarks unfortunately thoseproofs are still pretty big so they're at least hundreds of kilobytes they'd actually be into the multiple hundredsdepending on what exactly you want to base security upon I will discuss thislater in today's lecture another thing that a practitionerslike today about snarks in this category is that there's moreflexibility in what field you work over so any of the snarks that usecryptographic groups inside the polynomial commitment scheme so this means anything based on kzg anythingbased on bulletproofs or Dory or Hyrax these all use cryptographic groupsand cryptographic groups have to be defined over fields that are quite largeso at least about 256 bits in size if you want 128 bits of security in fact ifyou need to work over a pairing friendly group the the resulting Fields will be ofsize even like 381 bits that is the field has size 2 to the 381.so for example one of the most popular pairing friendly curves is called BLS 12381 and that 381 means thatthe field over which it's defined is of size about two to the 381.so if you use a polynomial commitment scheme such as FRI that does not use anycryptographic groups which is exactly why it's plausibly post Quantum secure you can work over potentially smallerfields and moreover those fields can be a little more structured and togetherboth the field size and the nice structure in the field can lead tofaster field arithmetic and therefore potentially speed ups for both theprover and the verifier I think it's worth pointing out two caveats regardingthis field size flexibility that the hashing based snarks have one caveat isthat if the field that one works over is too small say of size about 2 to the 64rather than of size say 2 to 128 or larger there are components of of theprotocol of the snark that will need to be repeated several times in order toachieve an adequate security level so that can potentially offset some of thebenefits of faster field arithmetic if you have to repeat some aspects of theprotocol multiple times another caveat is that many of the most important statements that are being proved today areactually statements about elliptic curve groups those statements arenaturally statements about very large Fields so if those are the statements that you are proving or that you arespending most of your time proving then there is very little point to being ableto work over smaller Fields because the best thing to do would be to work overthe large field that your statement is naturally about and in fact working over a different field will only slow thingsdown and add overhead so for example when transactions on the blockchainare very simple such as token transfers probably the most complicated aspect ofproving to the blockchain that you know a bunch of transactions is proving thatthe transactions contain valid digital signatures that is that the transactionswere authorized properly by the owners of the accounts affected by thetransactions the digital signature schemes used today are defined overelliptic curve groups and for this reason some of the Roll-Ups out theretoday are working over very large Fields such as 251-bit Fieldswhile using FRI so although fry gives the flexibility to work over smallFields it is very natural to nonetheless not take advantage of theflexibility if you are proving statements about large Fields because the statements you are proving refer toelliptic curve-based digital signature schemesHighlights of SNARK Taxonomy: Transparent SNARKsSpartan, Brakedown, Orion, Orion+okay a third important category of transparent snarks out there will combine various mip-based or IPbased polynomial iops with any polynomial commitment schemethat's particularly fast for the prover and this combination will give you thefastest prover out there today so examples here include Spartanbreakdown Orion and Orion plus. Orion plus is a part of a paper calledhyperplonk so for all of these protocols the underlying polynomial IOP is basedon the some check protocol we discussed in lecture four and each of these papers combines that polynomial IOP witha different polynomial commitment scheme meant to be fast for the proverand the yeah generally speaking the downside of these snarks right nowtoday at least is that their proofs tend to be bigger although actually Orion plusreports proof sizes under 10 kilobytes which in fact is much smaller than FRIbase proofs but it does so usinga variant to kzg Commitment so it in fact is not in the transparent snarkscategory despite me listing it on this slide okayHighlights of SNARK Taxonomy: Non-transparent SNARKSGroth16so that is a brief summary ofsome highlights on the in the category of transparent snarks I'll letme now briefly move to highlights of non-transparent snarks so one of the most popularnon-transparent snarks especially going back a couple yearsis Groth16 so this is sort of the culmination of a long line of workon snarks derived from linear pcps this is exactly the topic of nextlecture which upang will be given and the benefits of linear PCP based snarkssuch as Groth16 is that they have the best verification costs so grow 16proofs are only three group elements and the verifier just does a handful of pairing operations is typically theverification time bottleneck and the con the major con of growth 16is that not only is there a trusted setup but it's a circuit-specific trusted setup so if you change the circuit aboutwhich the prover is claiming to know a satisfying assignmentthen you have to run the trusted setup all over again it also tends to befairly slow and space intensive for the prover and due to its use of pairingfriendly groups or just any cryptographic groups even if they weren't paying friendly it is not post Quantum secureHighlights of SNARK Taxonomy: Non-transparent SNARKSMarlin-KZG, PlonK-KZGnow the other two highlights outthere of non-transparent snarks are Marlin and Planckboth referring to when they use the kzg polynomial commitment schemethe big benefit of the these over Groth16 is that the trusted setup is acircuit independent so you run the trusted setup once and that trustedsetup will work for any circuit up to a given size bound the downside of marlinand Plonk relative to Groth16 is that proofs are somewhat larger than Groth16 and the prover is slower than Groth16 atleast for a given circuit that Groth16 also applies to a CounterPointin terms of prover time is that Groth16 is somewhat restricted in the kindsof circuits it can use specifically it's sort of very specific to either r1cs orarithmetic circuits and all of the other snarks out there can actually apply to asomewhat more expressive kinds of circuits so it's possible that theslower a prover of marlin and Plonk relative to Groth16 for a givencircuit can be more than offset by the ability of marlin and Plonk toapply to a more General class of circuits and a slower prover applied to a smaller circuit might in the end befaster than a faster prover applied to a bigger circuit another thing people liketo say about Groth16 is that quote addition gates are free and what theymean is that the trusted setup in Groth16 which is circuit specific does haveruntime costs that grow with the number of addition gates in the circuit butonce the trusted setup is done the prover Groth16 essentially does notquote pay in runtime for any addition gates in the circuit that the prover isclaiming to know a satisfying assignment for whereas in Marlin and Plonk EditionGates would contribute to the prover timeFRI (Univariate) Polynomial Commitmentokay that completes my overview ofthe polynomial iops polynomial commitments and combinations that arepopular today and hopefully it clarifies why we are covering so many differentpolynomial iops and polynomial commitment schemes the design space isquite rich and understanding all of the possible performance profiles does requireunderstanding many different ideas and techniques there's just a lot of differenttrade-offs available out there okay so the rest of today's lecturewill be focused on Fry based polynomial commitments and more details about theFiat Shamir transformation then have been covered in previouslecturesRecall: Univariate Polynomial Commitmentsokay so let me remind you what is a univariate polynomial commitment scheme so we're going to let Q be apolynomial of degree K minus 1 over some prime field FP p is a prime so thatmeans that Q has K coefficients for example if K is 5 then Q might be oneplus two X Plus 4x squared plus x to the fourth so here the coefficient of theterm X cubed is zero and in a univariate polynomial commitment schemewe would like the prover to be able to commit to q and later reveal Q of R foran input R chosen by the verifier and the prover is going to have toprove that the returned valuation is indeed consistent with the committedpolynomialRecall: Initial Attempt from Lecture 4now you might remember from lecture four that I sketched a polynomialcommitment scheme that simply had the prover Merkel commit to all evaluationsof Q so that way when the verifier requested Q of R the prover could simplyreveal the associated Leaf of the Merkle tree along with its authenticationpath but this had two problems this did not actually directly give an interesting polynomial equipment schemethe first issue was that there is one leaf in the Merkle tree for every element of the field which means ifwe are working over a pretty large field the prover will just take way way toomuch time to build this Merkle tree and determine what is the root hash so we would really like the prover time togrow with the degree of Q and not with the size of the field which is typicallymuch much much larger than the degree of Q and secondlythis does not give the verifier any guarantee that the function that iscommitted that is the function whose evaluations are the leaves of the treeactually has degree at most K minus one Merkle trees do bind the prover to thevector given by the leaves but do not provide any information about structurewithin that Vector so the verifier in this sort of initial attempt at apolynomial commitment scheme from lecture four would have no guarantee at all that the function that the Merkletree binds the prover to is actually of degree at most K minus oneFixing the first problem (Want P time linear in degree, not field size)so let us fix the first problem regarding the prover time beinglinear in the degree of the polynomial and not the size of the fieldso rather than having the prover commit to all evaluations of Q acrossall inputs in the entire field FRI will instead have the prover onlycommit to evaluations of Q for those points X in a carefully chosen subsetOmega of the field specifically this set Omega will have size rho inverse times kfor some constant row so think of row as less than or equal to one half so rowinverse is at least two and this Factor row inverse infront of the degree bound K is sometimes referred to as the FRI blow upFactor row itself as opposed to row inverse is also called the rate of the readSolomon code used in FRI I will explain that in more detaillater now due to the prover Merkle committing to all evaluations of thecommitted polynomial Q over a set Omega of size ro inverse times kthere is a very strong tension between prover time and verification costs okay because the bigger the blowup Factor row inverse the bigger the prover time right because the prover has to spend the time to evaluate Q at allpoints in Omega so the bigger Omega is that is the bigger row inverses thelonger it takes the prover to evaluate Q at all those points and then build a Merkle tree over all those evaluationson the other hand as we will see the bigger the blowup Factor the smallerthe verification cost will be for FRI when the verifier receives an evaluationproof If the blow up factor is very big the proof will be shorter and theverification of the proof will be faster okay specifically here is how theverification costs in FRI depend on the blow up Factor row inverse so thisexpression will be a little bit technical but I'll explain how to think about it in a moment so the proof lengthand varied fire time is this many hash evaluations okay so here Lambda isWhat's called the security parameter also known as the number of bits of security at which FRI is configured soyou can think of Lambda as roughly the logarithm of the amount of work that acheating prover would have to do to convince the verifier to accept a falseclaim in FRI so that is the adversary will have to run in time about 2 to theLambda in order to find a convincing proof of a false statement and the sizeof the FRI evaluation proof will be roughly Lambda divided by log of theblow up Factor Times log squared of the degree of thecommitted polynomial many hash values so the way to think about what happens inFRI is that the verifier of FRI makes queries to the committed vector androughly speaking each individual query requires the prover to return logsquared k many hash values those are coming from Merkle authentication paths but in factthere are many Merkel authentication paths that need to be provided for each query and hence the number of hashvalues returned for each query is not log K as it would be with a singleMerkle tree but in fact is the square of that so then the question becomes howmany FRI verifier queries must be made to achieve Lambda bits of security andthe answer is roughly Lambda over log of the blow up Factor so that is eachFRI verifier query provides about log of the blow up Factor many bits of securityso in order to achieve Lambda bits of security the number of queries requiredis Lambda over log of the blow-up factor in summary the perverse time grows linearlywith the blow up Factor at least but the verifiers cost in terms ofboth proof length and verifier time Falls logarithmically with the blow upFactor so bigger blowup factor means more prover time but less verifiertime and shorter proofsThe key subset: roots of unityokay now let me tell you what is this subsetOmega at which fry requires the prover to evaluate the polynomial Q to becommitted so remember I said Omega has sizethe blow up Factor times the degree bound K so let me just give that size a name let me call it n and let's assume Nis a power of two which FRI will actually essentially require so whatOmega consists of is the set of all nth roots of unity in the fieldso that is the set of all field elements X such that if you raise x to the nth power you get out oneokay so this is how to think about roots of unity I have actually depicted here roots of unity over thecomplex numbers but in fact roots of unity over a finite field behavevery similarly to roots of unity for the complex numbers so I think theroots of unity the picture on the slide is exactly what you should have in mind as you try to think about roots of unityRoots of Unity visualizedused by FRI okay so this first image here is the 16th roots of unity amongstthe complex numbers okay so this represents the number one because thereal component is one and the complex component is zero and this represents the number minus one okay this is I thisis minus I okay so this root of unity right hereis What's called the Primitive root of unity for amongst the 16th roots of unityso this means if you take this primitive root of unity and you multiplyit by itself you get this root of unity if you take this root of unity and youmultiply It Again by the Primitive root of unity you get the next root of unity and every time you multiply Again bythis primitive root of unity you step to the what would this be counterclockwise one root of unityand after 16 multiplications so you take the Primitive root of unity and you raise it to the 16th power you get allthe way around to one now if we look at the eighth roots ofunity you see something very similar we have essentially just taken all of theyou know as we as we step around the circle we've taken the the odd steps so the Primitive 16 fruit of unityyou know that to the third power that to the fifth power that's the seventh power we'vegotten rid of them okay so like the Primitive a through to Unity is the Primitive 16th root of unity squaredand you know the next eighth root of unity is the square of that and so on and so forthand and then once again I've drawn the fourth root to Unity here andyeah we've taken sort of the the even indexed eighth roots of unity so this one's one you know this one's threefiveseven by that I mean just the order in which you encounter them as you step around the circle and they're gonethe odd ones are gone and the even ones remain okay so basically every time wetake a primitive root of unity and we Square it we get a primitive root ofunity for you know rather than n n becomes n over two and this isexactly the sort of behavior or the sort of that that is exploited byFRIThe key subset: roots of unityokay so let me just State some facts about the Primitive and through to Unity over a finite fieldeach of these facts except possibly the last one will exactly mirror the complex numbers picture I just showed on theprevious slide okay so if Omega is a primitive nth root of unitythat is n is the smallest integer such that Omega to the N equals one then this Capital Omega set of all nthrough to Unity in the finite field is just given by all powers of Omega so thezeroth power is one the first power is Omega itself and as you just increase that power two three four up to n minus oneyou step through all of the and through to Unity in the finite field the next fact is that the nthroots of unity form what's called a multiplicative subgroup of the field this just means that if X and Y are two nthrough to Unity then so is their product this is easy to seebecause the product X times y raised to the power n is the same as x to the Ntimes y to the n and since X and Y are both entered to Unity both x to the Nis 1 and y to the N is 1 as well and one times one is one so you take 2N through Unity you multiply them together yougetout another n through unity there are two special cases of this fact that are particularly important whichfollow from n being even okay the first is that if x is an nth root of unity then x squared is an N over two root ofunity this holds because obviously since n is even n over 2 is an integerand hence x squared to the power n over 2 is the same as x to the N which isjust one the second special case again since n is even is that forany nth root of unity x minus X is also an nth root of unity this holdsbecause clearly minus one is an nth through Divinity since n is even since minus one to any even power isone and so minus X is the same as minus one times x and you know by theclosure of the nth root to Unity under multiplication that means that minus X itself is a nth root of unityokay the final fact is also if you know I guess a littlebit of group theory is also a consequence of the second fact which isthat the set Capital Omega of and through to Unity has size n if and ononly if n divides P minus one okay so a basic fact from group theory is that thesize of any multiplicative subgroup of a multiplicative group divides the sizeof the full multiplicative group and the multiplicative subgroup of the field FPhas size exactly P minus one so the you can only have a non-trivial setof nth through to Unity if n divides P minus one and it turns out that theconverse is true as well so this is why many FRI based snarkswork over fields for which a very large power of two divides the field sizeminus one so for example a popular field today to work over when using FRIis the so-called Goldilocks field of size 2 to the 64 minus 2 to the 32 Plus 1. one reason this is so popular is that ifyou subtract one from the field size you get out 2 to the 64 minus 2 to the 32 which is divisible by 2 to the okay sobasically you can use within fry the N through toUnity for n equals you know 4 8 16 32 64 all the way up to 2 to the 32 No 2 tothe 32 is about 4 billionRoots of Unity: finite field examplelet me just give an example of the roots of unity used by FRI over a finite field rather than over the complex numbersso I will choose the prime field of order 41. so in this case the size of themultiplicative subgroup of this field which is specifically all non-zero field elements is 40.right and 40 is divisible by eight that is the largest power of two thatdivides 40. and so that means that there areeight a through to Unity in this field and we can kind of enumerate the eighthroots of unity by starting with the first root of unity and then moving to the two secondroots of unity you know which are just one and minus one because those are the only values in the field such thatif you square them you get out one then if you move to the fourth roots Unity you'll see there are four ofthem it's one minus one nine and minus nine so you can just check likenine nine to the fourth turns out to be congruent to one modulo forty oneso it requires a little bit of algebra but it is the case you know you can you can checkthis one of the facts I included on the previous slide was that ifX is an nth root of unity so is minus X as long as n is evenand you see that here so you know one is a four is minus one nine is a fourth root ofunity so is minus nine and then moving to the eighth roots in unity there are eight of them again eight divides P minusone and they are one and minus one nine and minus nine three and minus threeFourteen and minus fourteen okayFRI commitment to a univariateso summary as an example here if you were using FRI to commit to a polynomial Q over this field with 41 elements and theand you were using as the commitment set the set of a through toUnity the commitment to Q would simply be to take a q evaluated ateach of the eight a through to Unity Merkel hatched them and the root of the Merkle treeis the commitment to Q with FRI Fixing the second problemokay so that has fixed the first problem with our initial attempt at apolynomial commitment scheme from lecture four namely no longer does the prover time grow with the size of theentire field in this example 41 only does the prover time grow with thedegree of the committed polynomial K minus one times the fry blow upFactor but I just think of it as two two or four or sometimes it's 16 sometimes people will have it evenhigher but anyway think of it as some constant times the degree hence ingeneral the time for the prover to complete compute the commitment isyou know some constant only the fry blow up Factor times the degree instead of proportional to the size of theentirefield so in general the field might be vastly vastly bigger than 41elements it might have size say close to 2 to the 64 or close to 228 or evenclose to 2 to the 256. okay now there is a second problemthat we encountered with our initial attempt at a polynomial Crim scheme in lecture field for namely that theverifier needs to know that the Merkel committed Vector consists of allevaluations over this domain Capital Omega of n through to Unity ofsome polynomial of the appropriate degree okay remember Merkle trees do notgive you any promise I do not tell you anything about the structure of thecommitted Vector all they do is bind the prover to some vector and then enablethe prover to reveal entries of the vector to which it is bound it does not by itself tell you that that Vector hasany structure at all it's just some arbitrary Vector so the idea of fry is taken from theliterature on probabilistically checkable proofs the idea is that the verifier should inspect just a fewentries of the committed Vector to try to get a sense of whether it indeed has degree at most K minus 1 or not nowofcourse each query to the committed Vector will require the approver to not only respond with that particularcoordinate of the committed Vector but also a authentication path consisting oflog n hash values to prove to the verifier that thecoordinate returned by the approver is indeed consistent with the committedVector okay unfortunately if you just directly use these so-called low-degreetests from the probabilistically checkable proof literature in the context of fry they would be impracticalthe very verifier would unfortunately have to inspect a concretely very largenumber of entries of the committed vector and also the field one would haveto work over at least to get provable security from the low degree test wouldbe enormous so we do not directly use the low degree tests that were designedin the probabilistically checkable proofs literature from years agoinstead the fry low degree test will be interactive more specifically it willconsist of what I call a folding phase which will consist of logarithmicallymany rounds meaning the verifier will send the field element in round one and the prover will respond with anotherMerkel commitment then the verifier will send a second field element in round two and the approver will respond withyetanother Merkel commitment then the verifier will send another field element and the approver will respond withanother miracle commitment so on and soforth for log K many rounds and then finally the verifier will makethese fry verifier queries I referred to several slides ago when discussingverification costs and that query phase where the verifier makes itsqueries is only one round so the interactive part of the fry low degreetest primarily lies in the folding phase the query phase is minimallyinteractive the verifier just basically sends all of the queries it wishes to make and the prover responds withtheanswers and Associated Merkel authentication paths okay so let's talkabout what the folding phase does so in the folding phase the ideais to force the prover to repeatedly do the following the prover shouldrandomly fold the committed Vector in half so this means pair up entries of thecommitted Vector so pair up like the first and second entry the third and fourth the fifth and sixth then theverifier will pick a random field element R and use R to randomly combine every two paired up entries in this waythe length of the folded Vector will wind up being half the length of the original Vector because for every pairof entries in the original Vector we sort of randomly combine them into a single entry of the folded vector andwhat fry does is have the prover commit with a Merkle tree to the folded Vectornow the random combining technique is specifically chosen so that if the prover is honest the folded Vector willalso have half of the degree of the original Vector so not only should folding reduce the length of the vectorby a factor of two it should also reduce the degree of the polynomial that isyou know whose evaluations are given by the folded vector by a factor of two okay so if the original polynomial haddegree at most k then after we fold the that the original Vectorlog base two of K times the degree should fall to zeroat this point because the original Vector had length K times the fryblow up Factor while the you know this final Vector has degree zeromeaning if the prover is honest it is constant it's a constant functionthe length of this final folded Vector is at least two I mean specifically it's at least the fry blow up factorwhich isat least two so while this final folded Vectorhas two or more entries because its degree should be zero if the proveris honest the approver can actually specify the final folded Vector with a single field element right I need degreezero polynomial is best constant that is it Maps every element of itsdomain to the same output so that output is just some field element so ratherthan sending one field element for each entry of this final folded Vectorof which there are you know row inverse of these entries the provercan send just a single field element specifying the value that thatfinal folded function takes across its entire domain that domain having sizeequal to row inverse the fry blow up Factor here is a picture or exampleof the folding procedure used in Fry when the committed polynomial Q iscommitted using the a through to Unity from my example before over the fieldof size 41. so as you can see in the first foldthe entries of the initial committed Vector containing Q's evaluations at alla through to Unity are paired up with you know Q of one paired up with Q minusone Q of nine paired up with Q of minus nine and so forth and then based on the verifiersrandomly chosen field element R1 in that iteration of foldingthe each pair of paired up entries are combined do not worry right nowabout the precise coefficients used to do the combining that is chosenspecifically so that if the prover does the folding honestly the degree of Qwill reduce by a factor of 2 upon folding and then folding is done againand again until the resulting Vector should be all evaluations of a degreezero polynomial over the row inverse roots of unity again rho inverse is thefried blow up Factor at that point because the vector should be all evaluations of a degree zero function infact the prover if honest can just specify that Vector with this singlefield element so here I have depicted two folds okay now the Final Phase of fryas mentioned before is the query phase it's when the folds have all been done the intuition of the query phase isthatP may have lied at some step of the folding phase by not performing The Foldcorrectly so that is you know the approver is supposed to pair up entriesof the current Vector at each iteration of the folding procedure andcombine the paired up entries in a very specific way and the prover may not havedone that it may not not follow the prescribed floating procedure with the goal ofartificially reducing the degree of the claim folded Vector right because by theend of the folding procedure it has to wind up with a constant function thatis a degree zero polynomial so if the original committed Vector did not havedegree K minus 1 as claims but rather had much larger degrees say degree aslarge as n which is bigger than K by a factor of the fry blowup Factor the onlyway the approver is going to ultimately wind up with a degree zero function after log K manyfolds conceptually or intuitively is if the prover lies in some foldingprocedure and does not actually do the fold properly so the point of the query phase is for the verifier to try todetect any situation whereby the prover did not perform a fold correctlyso the verifier in the query phase will look for inconsistencies betweenyou know two two vectors one of which is supposed to be the the foldingof of the other so what this means is roughly the verifier will pick this many thismany entries of each of the you know committed vectors that are supposed to be foldings of the you knowprecedingVector this number is exactly the number that we discussed before whereby eachquery is providing about log of the blow up Factor many bits of security so we'regoing to need to pick about Lambda over log the of the blow up Factormany entries of each folded Vector to get Lambda bits of security and we need to confirm that for each selectedentryof each Vector indeed that entry is the appropriate linear combination ofthe relevant two entries of the previous vector okay so this leads to a proof sizedisplayed here what is happening is each of thefolded vectors of which there were log K of themis queried at this many points so the desired number of bits of securityLambda divided by log of the fry blowup Factorand then for each query there is a Merkel authentication path thatmust be provided along with the answer to the query the length of that authentication path is a log of thelength of the committed Vector that is being queried so the total number ofhash values sent is equal to the total number of queries to each of the log Kvectors with each authentication path being about log K hash values and thatgives you roughly this total quantity of hash values sent as the frysort of query phase okay so let's go into just a littlemore detail about how the folding phase actually works so the idea is that fry is going tosplit the committed polynomial Q into even and odd Parts where basicallythe even part will just take into account coefficients of evendegree and the odd part will take into account coefficients of odd degree okayso for example if Q of X were just 1 plus 2X plus 3x squared plus 4X cubedthen I'm saying the even component of Q would be one plus three x so this istaking the coefficients of even degree and sort of dividing the powers bytwo and then the odd component would be 2 plus 4X this is taking thecoefficients of odd degree and and again dividing the powers by two and takingthe floor that ensures that we can write the original polynomial Q as the evencomponent of Q evaluated at x squared plus x times the odd component of Qevaluated at x squared so we've literally just defined QE and qo sothat this equality holds okay so we think of sort of QE as capturinglike one half a Q and qo as capturing the other half a Q and we'd like torandomly fold these two halves together so what the verifier does is it picks a random field element R and sends itto pand they describe the folding of Q to be exactly what I just said so QE plus rtimes qo okay so I have described the folding procedure in this way to makeclear that the degree of cue fold is one half the degree of Q itselfthis is should be clear because we have split the original polynomial Qup into two halves QE and qo each of degree at most half thedegree of Q itself and taken a random linear combination of QEand qo with the kind of Randomness in the random linear combination given by this random field element R chosen bythe verifier and of course any linear combination of two polynomials both of degree at most n over two is itself ofdegree at both n over two so this way of defining cue fold makes very clear thatthe degree of the folded polynomial is at most half of the degree of theoriginal polynomial that is being folded but it is not necessarily obvious thatthe folding procedure as I've defined it here is equivalent to the foldingprocedure I depicted several slides ago whereby I said we paired up each nthroot of unity with its negative and took a acertain carefully defined linear combination of Q's evaluations at thosetwo and through to Unity so I now need to explain that thefolding procedure I just defined in terms of QE and qo is in fact equivalentto the folding procedure I depicted here in this picture so let us see thatequivalence now okay so yeah I justDefine the floating procedure in terms of QE and qoright it was QE plus r times qo is supposed to be the folded polynomial andI now want to explain the equivalent with the picture from before which amounts to showing thefollowing so if we let Z be x squared the picture I had from before showed thatQ fold of Z was this very specific linear combination of Q's evaluations atX and minus X so I'm just going to now explain that under the QE plus rqo definition ofq-fold you get exactly this Behavior whereby cue folds evaluation at anysquare of one of the nth roots of unity is this very specific combination of Q's evaluations at thetwo square roots of Z namely X and minus X so let's see why this equalityholds that is why this qeqo definition of cue fold impliesthis other definition of q-fold at each n over two root of unity Zokay so we Define QE and ql specifically so that evaluating Q at X wasequivalent to evaluating QE and qo at x squared and namely Zbecause Z is x squared multiplying the latter by X and summing the results so another way of statingthis equality is that in our qeqo definition of cue fold if we set R to Xthen qfold of Z is just exactly equivalent to Q of Xokay so we have now seen that under this definition of q-fold in terms of QE andqo if R were to equal x then qfold evaluated at Z would just be Q of xsimilarly if R were to equal minus X then qfold evaluated at Z would equal Qof minus X okay so in summary what these two bulletpoints right here show is that we understand what Q fold of Z would equal if R equals X or R equals minus Xokay we have introduced the right hand side hereexactly so that it has the correct Behavior at R equals X and R equalsminus X that is this right hand side is explicitly definedso that it Maps R equals x to Q of X and R equals minus minus X to Q of minus Xright you can just check that if R equals x what we have here is X Plus X over 2x that's 1 times Q of X and thenwhat we have here is x minus x times whatever x minus X is zero so x minus x timeswhatever is zero the right hand side of this equality right here hasexactly the correct Behavior when R equals X or R equals minus X and it isa degree one function of r and any two degree one functionsof R that agree at two or more points must be the same function accordinglywe have explained that this right hand side hereis in fact the expression forQ fold of Z regardless of R's value right whatWhat's Happening just to summarize is that we verified that this expression is true when R equals X or R equalsminus Xand because this is a degree one function in R and it takes exactly thesame values as this expression does at two different values of R thesemust actually be the same expressions yeah so that establishes theequivalence of this definition of q-fold in terms of QE and qo with thepicture I drew before which sort of explained or described cue folds Behavior at each n over tooth root ofunity Z as a certain linear combination of Q's Behavior at the two square rootsof Z namely X and minus X so I know this was a lot to follow online but youcan sort of take your time if you wish to or otherwise you can just believe me that we have two different equivalentdefinitions of cue fold one as a random linear combination of QE and qo and the other via the picture frombefore whereby the folded polynomial evaluated ateach of the N over 2 through to Unity is defined as a very specific linear combination of the originalpolynomial being folded at the two square roots of that n over 2 root ofunity okayso I just want to mention that fry is heavily exploiting the fact that thesquaring map is two to one when applied to the nth roots of unityif n is even this is why N is a power of 2 is anassumption of fry so that n will be even throughout the entire folding procedureyou know you keep dividing in by two it's still a power of two so it stays even the whole timeso this two to one nature of the squaring map on the nth root of unity iswhat ensures that the relevant domain after each fold has in size If insteadwe replace the nth roots of unity with some other set such as all of the field elements between 0 and N minus oneyouwould not have this property that squaring each of the field elements in the set would result in a set of halfthe size right so if you look at you know zero squared one squared 2 squared 3 squared up to n minus 1 squared youwould get out n different values at least as long as the characteristic ofthe field was bigger than n minus 1 squared that is not the case with the N through to Unity as you look at onesquared Omega squared Omega squared squared Omega cubedsquared so you square each of the nth roots of unity you don't actually get out anddifferent values you get out n over two values kind of each appearing twice sothat is a very important property of the nth Roots Unity that fry is exploitingthat property is not satisfied by other natural interpolation domains such askind of the first N Field elements okay so that is a completedescription of fry before I sketch why it is soundsand what you know talk about polynomial commitment schemes derived from fry let me compare it to thepolynomial commitment scheme such as lahero breakdown and Orion that we saw last lecture from upangokay so in upenn's lecture when he covered the error correctingcode-based polynomial commitment schemes those commitment schemes were very similar to Frye in that they all usederror correcting codes so fry uses something called the read Solomon code which involvesyou know taking a vector interpreting it as a univariate polynomial and evaluating that polynomial at manypoints in the finite field in this case the nth roots of unity and that's why I said before thatthis parameter row and Fry is also called the rate of the read Solomon code that's where that terminology comes fromis the coding Theory literature on read Solomon codes so the polynomialcommitment schemes in upenn's lecture were very similar in that they also use error correcting codes not necessarilyjust the read Solomon code and the only cryptography they used was hashing nowthe differences between the the lecture seven polynomial moments and Frywere as follows so the lecture seven seven schemes viewed any degree Dpolynomial as consisting of D to the one-half vectors each of length D to theone half and so they thought of the polynomial as sort of like a root dby root D Matrix and they performed what I call a single random fold of all thesevectors that is the verifier demanded that the prover send a a randomlinear combination of the rows of this Matrix okay so that's like one giant fold where you kind of combine root Dvectors each uses root d into a single Vector of length root d by taking a random linear combination of each ofthe vectors this winds up resulting in a polynomial commitment scheme withlarger proofs than fry namely size about root D but has some advantagesover fry for example you can achieve a linear time prover which fry does notbecause evaluating the committed polynomial Q at all n through to Unityrequires super linear time whereas if you use a a different errorcorrecting code than the read Solomon code you can hope for truly linear time also a breakdown is unique todayin that it works over any sufficiently large field unlike fry unlike thehero unlike Orion it does not have to work over a field where a very largepower of two divides the field size minus one okay in comparison to thelecture seven commitment schemes which kind of did one giant fold of many vectors fry views thepolynomial to be committed as a single Vector of length order D order its degree the hidden constant in this orderBig O notation is the fry blow up Factor rho inverse and what fry does is itrandomly folds that Vector in half logarithmic many times so rather than doing a single giant fold you know itdoes one fold in half to kind of have the size of the committed vector and the degree of the resulting polynomialand then it folds that in half and it folds that in half and it falls out in half until it's reduced the degree ifthe approvers being honestto zero so that's how I think you should compare the two conceptuallyeverything from last lecture did a single giant fold fry does logarithmically many folds in halfokay now let me sketch the security analysis of fry so recall that at the start of thefry polynomial commitment scheme the prover sends a Merkel commitment to a vector W which if theapprover is honest is equal to Q's evaluations over all n through to Unity where n is noticeably bigger thanthe degree of Q namely K minus oneso the verifier needs to check that indeed this committed Vector W hasdegree at most K minus one and that is sort of conceptually difficult to dothat's kind of the magic of fry without just reading all of W which would destroy succinctness of theevaluation proofs if the verifier needed to read the entirety of the committed Vectorso yeah in order to explain how the security analysis Workslet let Delta denote the relative Hamming distance of the committedpolynomial Q from the closest polynomial H thatdoes have degree at most K minus one so think of H as like the closest honestpolynomial to the actual committed polynomial Q okay so if the prover's lying the actualcommitted polynomial queue might have degree up to n but the closest honest polynomial is Hand by honest I mean the closest polynomial that actually does have degree K minus 1 which the proofer'sclaiming Q does but is lying possibly okay so by relative Hamming distancewhat I mean is Delta is the fraction of nth roots of unity such that H andQ do not agree at that input okay so that's called relative Hammingdistance that's the fraction of inputs over which you know Q's evaluations were provided at which Q disagreesfromthe closest honest polynomial h okay so the key claim is that theprover will pass all of the fry verifier querieswith this this probability okay so this is a degree of the honestpolynomial k k minus one divided by the field size plus 1 minus Delta tothe number of queries okay now each of these terms in this probability has a natural interpretationso this degree over field size is capturing the probability that theprover gets lucky in the folding phase and one of the folded vectors hasdegree much less than the vector that is being foldeddivided by two right so you know I said each fold should reduce the degree ofthe committed function by a factor of two okay if the prover gets reallyreally lucky the degree might actually Fall by a factor more than two and if that happens the prover might beable to reduce the degree all the way to zero after log K folds even though hestarted with something of degree n rather than degree k so this degree over field sizeprobability is bounding the probability that one of the folds is lucky forthe prover and when the verifier picks a random field element to do the fold with it's saying that there's sortof atmost k-meny field elements that the verifier might pick that reduce thedegree of the committed function by a factor more than two which would help a cheat improver okayand then you think of the second term as one minus Delta to the number of fryverify queries as the probability that the prover passes all of the verifiersconsistency checks in the query phase despite the fact that it did not get alucky fold that the cheating prover did not get a locking fold so this is like the probability that you know eventhough the it must have done at least oneof the folds incorrectly the verifier fails to detect the inconsistency during the query phase okayso some caveats about this claim before I explain why you should expect a claim to be true okay the firstcaveat is that this claim is only known to hold for proximity parameters Deltaup to 1 minus the square root of rho though it is conjectured to hold forDelta all the way up to one minus rho what this means is that we can onlyprove that each fry verifier query provides one halflog 1 over rho bits of securitywhereas it is conjectured that each Friv verifier query provides log 1 overrho bits of security most deep really I think alldeployments of fry analyze security under the conjecture sothat is they simply assume that each fry verify query provides log one over rhobits of security when in fact what we can prove is one half of that so onewould need to double the number of fry verifier queries thereby doubling thesize of the proof and doubling the verifier runtime in order toestablish you know the desired security level that these projects dodesire under you know an unconditionalresult okay so just as an example if the fry blow up factor is fourthen each fry verifier query is conjectured to contribute two bits ofsecurity but is only proven to provide one bit ofsecurity okay with the aforementioned caveats out ofthe way let me sketch how this claim is proven recall that the folded polynomial isdefined as a random linear combination of Q even and Q odd and itis really straightforward to check that if Q is Delta far from every degreeK minus 1 polynomial then at least one of QE or qo must be at least Delta farfrom every degree K over 2 polynomial over the N over 2 through to Unityso that is if both QE and qo were Delta close to low degree polynomialsover the N over 2 through its Unity it is not hard to check that Q itself would also be close to a degree Kpolynomialover the nth Roots Unity so the fact that the prover is lyingmeaning that Q is Delta far from every degree K minus 1 polynomial H tells usthat at least one of QE or qo is also Delta far but now with the relativeHamming distance defined over the N over 2 through to Unity so the key idea isthat a random linear combination of two functions Q even and Q odd at least oneof which is Delta far from degree D polynomials will also be Delta far from degree D with overwhelming probabilitywhere the probability here is over the random choice of the linear combinationremember the folded polynomial was defined as QE plus r times qo so theprobability that this idea refers to is just over the random choice of rokay so quantitatively this K Over P term where p is the field sizebounds this probability that the cheating prover gets a lucky R wherebyQ fold winds up being closer to a polynomial of degree less than K over 2then it should be given that Q is not close to any polynomial of degree atmost k okay so that is the first idea whichbasically says that the prover is very unlucky to quote get a lucky foldthe second idea is that if the approver does not get a lucky fold then the truefinal folded function will basically by definition be at leastDelta Far From Any degree zero function but fry forces the prover to send a degree zero function in the final roundof the floating procedure so at least one fold had to have been done dishonestly by the cheating prover andin this case each fry verifier query will detect some inconsistency withina fold meaning it will detect that the folded Vector is not the appropriatelinear combination of the paired up entries of the vector that was foldedwith the probability it will detect this is at least this proximity to parameterDelta so the probability that every single one of the fry verify queriesfails to detect any inconsistency in is at most 1 minus Delta to the numberof queries right it's saying that any particular query fails to detect the inconsistency with probability one minusDelta so the probability that all of the queries fail to detect the inconsistencyis at most 1 minus Delta raised to the power number of queries so that is theidea behind the soundness analysis of fry now I do want to mention theknown attack on Fry which shows that the conjectured soundness of fry if true istight so here is the known attack so recall at the start of the frypolynomial commitment scheme the prover Merkel commits to a vector which isclaimed to equal the polynomial to be committed evaluations over the nth rootsof unity here n is larger than the claim degree bound K by a factor equal to thefry blow up Factor rho inverse so the following prover strategy works for anypolynomial Q even one that is maximally far from degree K and this proverstrategy will pass all of the fry verifier checks with probability atleast rho to the number of five error fire queries t Okay so this is sayingthis previous strategy Works no matter how big a lie the prover is telling so no matter how far the committedfunctionQ is from a degree K polynomial the following prover strategy will succeedin convincing the verifier to accept the false claim that Q has degree K withprobability at least rho to the T so here is the strategy the prover picks aset capital T of K elements of the nthroot to Unity and interpolate a polynomial let me call it s of degree Kminus 1 that agrees with Q just at those points in capital T okay so s is goingto agree with Q at exactly K points k n through to Unity and we're notgoing to control s's Behavior at any other points whatsoever now when duringthe folding phase when in the very first round of the folding phase whenthe prover is supposed to fold Q it instead folds sokay at all other folds it answers honestly so the very first fold is donedishonestly meaning it is done with evaluationsof s at the nth roots of unity rather than with Q at the ends with nth rootsof unity and then from then on the prover throughout the entire protocol behaves honestly andthe point is that this proving strategy will pass all of the verifiers checks ifevery one of its T of the verifiers T queries lies in the set capital T atwhich s and Q agree okay and because we are looking atthe nth root to Unity and S and Q agree at a row fraction ofthose nth roots of unity each individual five Eric fryer query lies in theagreement set with probability rho all little T fry verifier queries lie inthe agreement set with probability at least row to the T okay so to summarizeif the conjectured soundness of fry is correct the conjecture would be tightthere would be no hope of improving it because there is a very simple attackthat achieves success probability row to the number of fry verifier queriesregardless of how big a lie the prover is telling by which I meanthis attack works for any committed function queue no matter howfar it is from a degree K polynomial of course the prover is claiming it hasdegree K and I am saying that this attack will work no matter how untruethat claim might be okay now let's finally talk aboutgetting a polynomial commitment scheme from Frye because believe it or not evenafter all of this time I spent explaining fry we've not actually seen a complete polynomial commitment schemefrom it so remember going back to our initial attempt at a polynomialcommitment scheme from lecture four the prover Merkel committed to all evaluations of the polynomial Cube to becommitted meaning every evaluation from the entire field and this meant thatwhen the verifier in this attempted polynomial commitment scheme requested the evaluation of Q at some point R inthe field the approver could just reveal the associated Merkle Leaf along with the authentication information butnowwe have the problem that Frye only has the prover Merkel commit to evaluations of Q amongst the nth roots of unitynotover the whole field so that is an issue be because the verifier May request anevaluation of Q at any point R in the field not just among the nth Roots Unityso it's not going to be enough for the prover to Simply reveal some Leaf of theMerkle tree along with authentication information since if R is not an nth root of unity the Q evaluated at R isnot a leaf of the merko tree that is just one problem another problem is thatthe verifier does not know after applying fry to q that Q is exactly lowdegree all it winds up being convinced of is that Q is quote not too far fromlow degree meaning that the verifier after applying off the fry low degreetest is convinced that the relative Hamming distance of Q from a degree Kminus 1 polynomial is not too big but it does not know that that relative Hammingdistance is zero so we have to address both issues so the way to address thisis relies on the following fact which was also used in kcg commitmentsso it is that a degree d univaripolynomial Q satisfies the property that it evaluates to V at input R if and only if thereexists a polynomial W of degree at most D such that Q of x minus V is equal to wtimes x minus r okay so it is you know easy to see thatif if Q of R equals V okay thenQ of x minus V will be divisible by x minus rbecause both the left hand side and the right hand side will evaluate to zero atR and so w would just be the quotient polynomial in that case and it is alsoeasy to see that if Q of R does not equal V then we cannot possibly havesuch a w because the left hand side at R would evaluate to something non-zerowhile the right hand side would evaluate to zero which would be a contradiction hence in order to confirm that Q ofRequals V where Q is the committed polynomial the verifier can apply Fry'sfold and query procedure to the functionobtained by taking the equation above and multiplying both the left hand sideand right hand side by x minus r inverse using degree bound D minus one so thateffectively checks that this polynomial W in the equation up here indeed existsand has degree at most D minus 1 which the fact that we discussed aboveexplains is equivalent to the committed polynomial Q evaluating to V at input Rso note that in order to apply fry tothis function here the fry verifier we will have toquery this function at various nth roots of unitywhen the fry query procedure is appliedand any evaluation of this function can be obtained with just asingle query to queue at the same point the verifier then derives the thisfunction evaluated at that point by simply subtracting V from it and thenmultiplying the result by you know the evaluation Point minus r inverseand it is not difficult to show that in order for the approver to pass theverifiers checks in this polynomial commitment scheme with noticeable probabilitythe it better be the case that the claimed evaluation of the committedpolynomial Q at R is equal to H evaluated at R where H is the degree Dpolynomial that is closest to Q so this polynomial commitment schemekind of has the cool property that even though the verifier doesn't know for sure that the committed Vector Qis exactly low degree is exactly degree at most Dwhere in my earlier notation D was K minus 1 nonetheless for the prover to passthe verifiers checks with noticeable probability in this commitment scheme whereby thefry low degree test is applied to this derived polynomial derived from therequested evaluation point and the claim the valuation then the approver has tohave the claimed evaluation equal to an actual exactly low degree polynomial Hevaluated at the requested point R so effectively the prover is bound toanswer evaluations to Q consistently with the closest truly low-degreepolynomial H to Q okay so a caveat here is that the securityanalysis of this polynomial commitment scheme requires the relative Hamming distancewhich with which the fry low degree test is applied and analyzed to be atleast this quantity one minus rho over two this is called the unique decoding radius of the read Solomon codethe upshot is that each fry verifier query will bring less than one bit ofsecurity to this polynomial commitment scheme okay so what's happening is thattoday people are using fry actually as a weaker primitive than thepolynomial commitment scheme described in this slide they are using it assomething that's more properly referred to as a list polynomial commitment schemeit turns out that a list polynomial commitment schemes still suffices for snark security so a list polynomialcommitment scheme effectively does not bind P to a single low degree polynomialas required by a polynomial commitment scheme but instead bounds P to a smallset of low degree polynomials in the sense that when the verifier requests Qof R the prover does not necessarily have to answer consistent with h of Rfor a fixed low degree polynomial H butinstead is able to choose from a small set of low-degree polynomials and answerwith any one of those polynomials from the small set evaluated at R and thisso-called list polynomial commitment scheme turns out to be enough for snarksecurity so it is a bit of a simplification or a lie to tell you thatevery single snark out there combines a polynomial IOP with a polynomialcommitment scheme in fact it often suffices to use a list polynomial commitment scheme at any rate so peopleare using this weaker primitive called a list polynomial commitment scheme inorder to be able to conjecture that each fry verifier query provides a log of 1over rho bits of security rather than less than one bit of security andthereby keep the verification costs smaller than if they used a fry as atrue polynomial commitment scheme that binds the prover to a single polynomialof a specified degree bound okay now let us talk in detail about the FiatShamir transformation in concrete security because when fry is deployedtoday in blockchain applications it is as far as I know always deployednon-interactively by applying Fiat Shamir to it and of course the Fiat shimir transformation is used in almostall snarks with the notable exception of growth 16 every other snark in use today as far asI am aware there might be some small exceptions that are related to grow 16 will take an interactive succinctargument and render it non-interactive with Fiat Shamir so let us discuss FiatShamir in more detail than any of the previous lectures okay so I am going toremind you how Fiat Shamir works I am going to describe it for Simplicity inthe context of a three message in interactive protocol where the approver speaks first so in the interactiveprotocol the approver sends a message Alpha to the verifier who responds with a random challenge beta and then theprover responds to Beta with a final message gamma now in order to renderthis interactive protocol non-interactive what the fiatrimir transformation will do is effectivelyreplace beta with a hash evaluation withthe hash function modeled as a random Oracle R so specifically rather than theverifier choosing beta and sending it sending it to the prover beta will be chosen by evaluating therandom Oracle think of that as a cryptographic hash function at the approvers first message in the protocolAlpha now if you want the resulting non-interactive argument to satisfy something called adaptive security whichmeans it is secure even if the the adversary can select the input X fed tothe verifier you better include x in the Tuple being hashed in addition to theprover's first message this is actually a mistake that has appeared many timesover many years including recently in many snark deployments and so please don't make that mistakeyourself if you omit X that X here is the verifiers public input if you omitit from the FIA chamir hashing you will not have adaptive security that is you will not have security againstapproverthat can choose X adversarially for the non-interactive argument Okay sowhenever you apply the Fiat Shamir transformation the following attackis always available for a cheating prover so this is often called thegrinding attack on fiatrimir so a cheating prover making a false claimCan iterate over all first messages Alpha until it finds onethat hashes to a lucky verifier challenge beta by a lucky verifierchallenge beta I mean one for which the cheating prover can efficiently find aresponse gamma that will convince the verifier to accept Okay so in thisgrinding attack right the prover just you know it it it tries you know somefirst message Alpha sees if the resulting hash value is lucky if not ittries another Alpha if if that next Alpha doesn't get hashed to somethinglucky it just tries another Alpha and another and another just keeps trying until it finds a lucky Alpha okay so asan example suppose you apply the Fiat Shamir transformation to an interactiveprotocol with 80 bits of statistical security by which I mean the soundness error of the interactive protocol is atmost 2 to the minus 80. this this grinding attack will have achieved the following trade-off between workexpended and success probability of completing the attack so with 2 tothe B hash evaluations the grinding attack will succeed with probability2 to the minus 80. plus b okay so for example with 2 to the 70hashes a cheating prover can successfully finda convincing proof for any false statement with probability about 2 to the minus 10 which is about one over onethousand okay so basically every time the prover tries a another first messageAlpha it has a probability of about 2 to theminus 80 of that Alpha being lucky meaning it hashes to a lucky Challengeand it only has to get lucky once so this particular trade-off betweencomputational effort expended by the attacker and success probability of theattack should be contrasted with the trade-off achievable for othercryptographic Primitives configured to the same quote security level so forexample with a collision resistant hash function configured to 80 bits of security the fastest Collision findingprocedure should be a birthday attack if the hash function is well designed andthe computational effort versus success probability trade-off here looks quitedifferent than when attacking the Fiat Shamir transformation so specifically when a birthday attack performed two tothe K hashes when attacking a hash function configured to 80 bits of security itwill succeed in finding a collision with probability of only two to the 2K minus1 6 60. so this is quadratically worse than the success probability of thegrinding attack against Fiat Shamir so for example with 2 to the 70 hash evaluationsthe birthday attack will find a collision in a hash function configuredto 80 bits of security with a probability of only 2 to the minus 20 which is about 1 over 1 million ratherthan 2 to the minus 10 which is about one over one thousand and this difference in successprobabilities for a given amount of computational effort can make a dramaticdifference in the expected profitability of an attack when the attacker hasbudget limitations that prevent it from expending enough work to succeed withprobability close to one so if an attacker is limited to say 2 to the 70hash evaluations it is certainly going to matter a lot to that attacker whendetermining whether or not to launch an attack whether the attack will succeed with probability 2 to the minus 10versus 2 to the minus 20. so the point here is that when protocol designersrefer to 80 bits of security or Lambda bits of security in general they arereferring to the logarithm of the number of computational steps required tosucceed in an attack with probability close to one but that does not specifyhow quickly the success probability of the attack increases as the amount ofcomputational effort invested in the attack increases and in fact for thegrinding attack on the Fiat Shamir transformation the situation isconsiderably worse if you are a protocol designer looking to design a secureprotocol then the situation for many other cryptographic Primitives includingas we've discussed here finding collisions inclusion resistant hash functions and breaking the discretelogarithm problems that is finding X from G to the X for a group generator GI think an important question that any protocol designer will encounter is howmany bits of security is enough and in order to answer that question one mustunderstand how many hash evaluations are feasible today and at what price so allI can do in this lecture is provide just a couple of data points to give you a sense so today the Bitcoin Networkperforms just about 2 to the 80 shot 256 hashes every hour or so and atcurrent bitcoin prices those tip those hashes typically earn less than a million dollars worth of block rewardsnow obviously the world as a whole has invested enormous sums of money intoAsics to make Computing these hashes as fast as possible butnonetheless this should give some sense that with enough investment inAsics at least 2 to the 80 hash evaluations are in fact quite cheap andcan be done quite quickly as another data point a research paper reportedthat in January 2020 about out 2 to the 64 sha-1 evaluations cost about fortyfive thousand dollars on gpus that would put two to the 70 Xiao one evaluationsat about three million dollars and today the rental of gpus is likely tobe cheaper than it was in January 2020 when many of those gpus wereprofitably being used to mine ethereum and today they will not beokay so that should just give a sense of why standard cryptographic practiceis to set the number of bits of security to be a 128 or more that puts the numberof hash evaluations used to attack a Fiat Shamir based snark set to thatsecurity level substantially beyond the reach of current hardware and alsoleaves a bit of a buffer in case better attack attacks are found than what are known todayfinally let me discuss or different security levels might be appropriate ifa protocol is being run interactively versus if it is being Fiat shamired sothat it is can be run non-interactively now a polynomial commitment schemesuch as fry run run interactively at Lambda bits of security has thefollowing guarantee so assuming the cheating prover cannot find a collisionin the hash function used to for Merkel hashing then a lying prover cannot passthe verifiers checks with probability better than two to the minus Lambda now a key Point here is that a a cheatingprover attempting to attack the interactive variant of fry must actuallyinteract with the verifier to learn the verifier challenges and thereby find outwhether the attack will succeed okay put another way the approver cannot know ifit's at attack will succeed until it actually speaks to the verifier to learn what the verifier challenges areokay as an example if fry is run interactively at 60 bits of securitythen with probability at least one minus one over a billion or so the verifierwill reject at least one billion times before the cheating prover succeeds itseems unlikely that a verifier would continue interacting with a prover thathas been caught in a lie over one billion times moreover the interactive protocols mightsimply take too long to run for any prover to execute over a billion attacksin a reasonable amount of time so as one data point one billion ethereum blocksshould require about three years to create assuming ethereum is creating blocks at about one block per 12 secondsokay in contrast if you apply Fiat Shamir to an interactive protocol such as fry that was run at Lambda bits ofinteractive security before fiatrimir was applied the resulting non-interactive protocol is a muchweaker security guarantee essentially the key point is that a lying prover canattempt the grinding attack silently so unlike in the interactive casethe prover can basically just perform something resembling you knowthe interactive attempts at attacks in its own head without revealing to the verifierthat the attack is occurring so rather than the actual verifier having toreject the prover over one billion times for the prover to have even a one in a billion chance to succeed theprovercan just attempt the grinding attack you know meaning with one hash evaluation required per attempt twoto the 60 times which is entirely feasible on Modern Hardwarewithout any verifier ever knowing that the approver was attempting anattack in the first place as a result much higher security levels are requiredin the non-interactive setting than in the interactive setting so for example60 bits of security might be totally fine in the interactive setting unlessvery very very large sums of money on the line think multiple billions of dollars perhapswhereas in the non-interactive setting 60 bits of security is not going tobe enough unless the payoff is really minimal because again 2 to the 64 sha-1 hash evaluations three years agocost under forty five thousand dollars finally I want to discuss an issue thatwhile well known I feel may be nonetheless underappreciated today andthat is that the Fiat Shamir transformation can lead to a tremendous loss of security when it is applied tomany round interactive protocols to render them non-interactive in fact whenapplying the fiatrimir transformation to some many round protocols that have negligible soundless error theresultingnon-interactive protocol may be entirely insecure here is the canonical exampleof such an interactive protocol consider the empty language so this means thatthe verifier should always reject literally nothing should be accepted by the verifier and let us consider thefollowing somewhat silly interactive protocol for the empty language so theprover in this silly interactive protocol sends a single message whichI'm going to call a nonce because the verifier is just going to ignore the prover's messageso after the verifier receives and ignores the prover's message the verifier then tosses a coin and Haltonoutputs reject if the coin comes up heads but Halton outputs except if thecoin comes up tails okay now the soundness error of this protocol is one half because withprobability a half the verifier winds up accepting when it really shouldnever accept Okay now if you sequentially repeat this protocol Lambda times and accept only if everyrepetition of the base protocol accepts the soundness error Falls from one half and one repetition to one over twotothe Lambda because the probability that all Lambda runs except is exactly one over two to the Lambdanow consider applying Fiat Shamir to this lambda-round protocol obtained bysequentially repeating the soundness era half protocol Lambda timesapplying Fiat Shamir to this protocol will render the protocol non-interactivehowever the resulting protocol is totally insecure in fact a cheating prover can find a convincing quote prooffor the non-interactive protocol with only order Lambda hash evaluations thekey idea of the attacker is to execute a grinding attack on a single repetitionof the base protocol at a time rather than attempting to quote grind upon allLambda repetitions all at once so this is what I mean by that so the attackercan grind on the first repetition of the base protocol alone first that meansiterating over nonses that the prover might send in the first repetition of the base protocol on until one is foundthat hashes to Tails so in expectation the attacker only has to try two different nonses before it's going tofind one that hashes to Tails once it finds one that hashes details it can fix that nonce let's call it M1 for theremainder of the attack once it has succeeded in finding M1 it can then execute a grinding attack on the secondrepetition of the protocol by searching for a nonce M2 such that M1 comma M2hashes to Tails once it finds such an M2 it will have effectively passed theverifiers checks in the first two repetitions of the base protocol it canfix M2 and move on to finding an M3 such that M1 comma M2 comma M3 hashes totails in this way it can find a successful nonce Mi for each repetitionI of the base protocol iteratively it does not conceptually it does not haveto guess lucky M1 M2 up to M Lambda likeall at once it can sort of grind until it finds an M1 that's lucky then it can grind until it finds an M2 that'sluckythen it can grind until it finds an M3 that's lucky and so on and so forth and in expectation only two attempts atfinding each Mi are required so the takeaway from this example isthat applying the Fiat Shamir transformation to many round interactive protocols can lead to a huge loss ofsecurity whereby the resulting non-interactive protocol is totally insecure okay now it is known that ifthe many round interactive protocol can be shown to satisfy a strongernotion of soundness called round by round soundness then in fact it is safe to apply FiatShamir to the many round protocol that is showing that an indirect protocol satisfies round by round soundnessandrules out the kind of catastrophic security loss that applying Fiat Shamir to this kind of silly sequentialrepetition of the base protocol for the empty language demonstratedintuitively what is round by round soundness a protocol is round by round sound if anattacker in the interactive protocol in order to convince the verifier toaccept has to somehow get lucky all at once in sort of a single round it can'tkind of space out its luck across many rounds by getting a little bit Lucky in many different rounds okay so thissequential repetition of a soundness one half protocol that I presented before is the canonicalexample of a protocol that is not round by round sound in the sequential repetition which is not round by roundsound we saw that the attacker can get a little Lucky in each round and succeed right it only has to get theverifier totoss tails in each round so in each round it gets lucky sort of with probability of half and it manage if itmanages to string Lambda many you know slightly lucky rounds together it manages to convince the interactiveverifier to accept so that is the epitome of not being round by round sound there are our example protocolsknown that are round by round sounds despite having many rounds a key exampleis the some check protocol that I covered in detail in lecture four this is a logarithmic round protocollogarithmic sort of in the size of the statement being proven and it is known to be round by round sound so theproverb kind of has to get very very lucky in a single round it can't get a little bit Lucky in each and every oneof the logarithmic rounds and hence we know that applying the Fiat Shamir transformation to the subject protocolleads to a sound non-interactive protocol in the random Oracle model something similar is known forbulletproofs this is a logarithmic round proof of knowledge that's commonly usedas a polynomial commitment scheme which upang discussed I think two lectures ago so one one thing I do want tomention isthat fry is a logarithmic round interactive protocol that as far as I know is is currently deployed exclusivenon-interactively by applying the fiatrimir transformation to it and it has not been shown to be round by roundsound at least this has not been published so this is kind of a gap in the known security analyzes which I'mactually hoping to patch soon with with a new preprint so it's not that I think fry is not round by round soundbut I do think that for a pervasively deployed protocol such as fry which has deployed non-interactively we shouldknow for sure that it's round by round sounds to rule out the possiblecatastrophic loss in security that we know is possible when applying FiatShamir to many round protocols so as a general takeaway of this coverage of fiatrimir any protocoldesigners that are obtaining a snark by applying the fiatrimir transformation toan interactive protocol was strictly more than three messages should show that the protocol is round by roundsounds if they want to rule out a major loss of security that comes from theapplication of the fiatrimir transformation to the many round protocolI do think there is an under emphasis on such analyzes today whereby protocoldesigners are simply assume that there is zero loss of security whenapplying the Fiat Shamir transformation even to protocols with strictly more than three messages when that can onlytruly be justified if it is shown that the interactive protocol is round byround sounds otherwise even in the random Oracle model there might be asignificant loss of concrete security when Fiat Shamir is applied okay that is the end of today's lecturenext lecture will cover snarks from linear pcps such as growth 16 thefollowing lecture will cover snark composition and recursion and after that I believe we are turning to Applicationsof snarks thank you very much